{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygwalker\n",
    "#!pip install ydata_profiling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygwalker as pyg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import ydata_profiling\n",
    "from scipy.stats import skew\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea8215b",
   "metadata": {},
   "source": [
    "# EDA + Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538e197",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visualizing EDA in a Tableau-like interface\n",
    "kickstart = pd.read_excel(\"E:/Users/pc/Downloads/Mining proj/Induvidual proj/Kickstarter.xlsx\")\n",
    "walker = pyg.walk(kickstart)\n",
    "walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "##INTERACTIVE EDA USING PANDAS PROFILING####\n",
    "profile = ydata_profiling.ProfileReport(kickstart)\n",
    "profile.to_file(output_file=\"E:/output2016_20.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text (basic steps)\n",
    "kickstart['name'] = kickstart['name'].fillna('').str.lower()\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(kickstart['name'])\n",
    "\n",
    "# Function to impute missing values\n",
    "def impute_category(row):\n",
    "    if pd.isna(row['category']):\n",
    "        # Compute similarity\n",
    "        similarities = cosine_similarity(tfidf_matrix, tfidf_matrix[row.name])\n",
    "        \n",
    "        # Get indices of most similar rows\n",
    "        similar_indices = np.argsort(similarities[:,0])[-3:-1]  # Adjust as needed\n",
    "\n",
    "        # Impute using the mode of the most similar rows\n",
    "        similar_categories = kickstart.iloc[similar_indices]['category']\n",
    "        if not similar_categories.empty:\n",
    "            mode_result = similar_categories.mode()\n",
    "            if not mode_result.empty:\n",
    "                return mode_result.iloc[0]\n",
    "            else:\n",
    "                return np.nan\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return row['category']\n",
    "\n",
    "# Apply imputation\n",
    "kickstart['category'] = kickstart.apply(impute_category, axis=1)\n",
    "\n",
    "#Display categories after imputing\n",
    "kickstart[\"category\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af39a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting goal to goal_usd\n",
    "kickstart[\"goal_usd\"] = kickstart[\"goal\"] * kickstart[\"static_usd_rate\"]\n",
    "\n",
    "#Drop invalid predictors\n",
    "kickstart = kickstart.drop([\n",
    "    'id', 'name', 'goal', 'pledged', 'deadline', 'created_at', 'launched_at', 'staff_pick',\n",
    "    'usd_pledged', 'backers_count', 'static_usd_rate', 'name_len', 'blurb_len',\n",
    "    'state_changed_at', 'state_changed_at_month', 'state_changed_at_day',\n",
    "    'state_changed_at_yr', 'state_changed_at_hr', 'spotlight',\"currency\",\n",
    "    \"disable_communication\",\"state_changed_at_weekday\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5304533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing cancelled and suspended rows in state column\n",
    "kickstart = kickstart[kickstart['state'].isin(['successful', 'failed'])]\n",
    "\n",
    "# Counting missing values in each column\n",
    "missing_values = kickstart.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]  # Filter columns with missing values\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=missing_values.index, y=missing_values.values)\n",
    "plt.title('Missing Values Count per Column')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Missing Values Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only have 150 something missing values in only category column, so we will remove that. (only 1%)\n",
    "#Removing missing values\n",
    "kickstart = kickstart.dropna()\n",
    "kickstart = kickstart.reset_index(drop=True)\n",
    "kickstart.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2476db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation heatmap\n",
    "plt.figure(figsize=(16, 8)) \n",
    "sns.heatmap(kickstart.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking multi colinearity\n",
    "correlation_matrix = kickstart.corr()\n",
    "\n",
    "# Filter out pairs with correlation greater than 0.8\n",
    "high_correlation_pairs = [(i, j) for i in correlation_matrix.columns for j in correlation_matrix.columns \n",
    "                          if (i != j) and (abs(correlation_matrix[i][j]) > 0.8)]\n",
    "\n",
    "print(\"Pairs with high correlation (> 0.8):\")\n",
    "print(high_correlation_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove correlated variables\n",
    "kickstart = kickstart.drop(['deadline_yr','created_at_yr','launch_to_state_change_days'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = kickstart.corr()\n",
    "\n",
    "# Filter out pairs with correlation greater than 0.8\n",
    "high_correlation_pairs = [(i, j) for i in correlation_matrix.columns for j in correlation_matrix.columns \n",
    "                          if (i != j) and (abs(correlation_matrix[i][j]) > 0.8)]\n",
    "\n",
    "print(\"Pairs with high correlation (> 0.8):\")\n",
    "print(high_correlation_pairs)\n",
    "\n",
    "\n",
    "#There is no colinearity now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbac7a",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-tabulation\n",
    "cross_tab = pd.crosstab(kickstart['country'], kickstart[\"state\"])\n",
    "print(cross_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94437afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify major countries with a significant number of projects\n",
    "major_countries = ['US', 'GB', 'CA']  # Based on the initial data overview\n",
    "\n",
    "# Step 2: Create a function to categorize countries\n",
    "def categorize_country(country):\n",
    "    if country in major_countries:\n",
    "        return country\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Step 3: Apply the categorization function to the 'country' column\n",
    "kickstart['country_grouped'] = kickstart['country'].apply(categorize_country)\n",
    "\n",
    "# Step 4: Remove original country column\n",
    "kickstart = kickstart.drop([\"country\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the occurrences of each category\n",
    "category_counts = kickstart['category'].value_counts()\n",
    "\n",
    "# Calculating the percentage of each category\n",
    "category_percentage = (category_counts / category_counts.sum()) * 100\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(26, 6))\n",
    "sns.barplot(x=category_percentage.index, y=category_percentage.values)\n",
    "plt.title('Percentage Distribution of Categories')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55eadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the occurrences of each category\n",
    "category_counts = kickstart['category'].value_counts()\n",
    "\n",
    "# Calculating cumulative percentage\n",
    "category_cumulative = category_counts.cumsum() / category_counts.sum() * 100\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_cumulative.plot(kind='bar')\n",
    "plt.title('Cumulative Percentage Distribution of Categories')\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Cumulative Percentage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using top 8 categories we are covering 80% of the distribution, so the rest will be classified as others.\n",
    "\n",
    "# Extracting the top 8 categories from the 'category' column\n",
    "top_8_categories = kickstart['category'].value_counts().nlargest(8).index.tolist()\n",
    "\n",
    "# Categorization function to categorize based on the extracted top 8 categories\n",
    "def categorize_based_on_top_8(category):\n",
    "    if category in top_8_categories:\n",
    "        return category\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the categorization function to the 'category' column\n",
    "kickstart['category_grouped_top_8'] = kickstart['category'].apply(categorize_based_on_top_8)\n",
    "\n",
    "# Display the first few rows of the dataset to verify the changes\n",
    "kickstart[['category', 'category_grouped_top_8']].head()\n",
    "\n",
    "#Removing and assigning grouped values to the original category column\n",
    "kickstart = kickstart.drop([\"category\"],axis=1)\n",
    "kickstart[\"category\"] = kickstart[\"category_grouped_top_8\"]\n",
    "kickstart = kickstart.drop([\"category_grouped_top_8\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261771e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of 2 separate columns. I created a column representing number of days from creation till deadline\n",
    "kickstart[\"create_to_deadline_days\"] = kickstart[\"create_to_launch_days\"] + kickstart[\"launch_to_deadline_days\"]\n",
    "\n",
    "#Removing launch_to_deadline and creation_to_launch because they are not needed\n",
    "kickstart = kickstart.drop([\"create_to_launch_days\",\"launch_to_deadline_days\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ed52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding for the solumns that have categories in text-format\n",
    "\n",
    "for col in ['launched_at_weekday', 'created_at_weekday', 'deadline_weekday', 'category', 'country_grouped',\"state\"]:\n",
    "    # Generate dummy variables\n",
    "    dummies = pd.get_dummies(kickstart[col], prefix=col)\n",
    "\n",
    "    # Drop the first column of the dummy variables to avoid multicollinearity\n",
    "    dummies = dummies.iloc[:, 1:]\n",
    "\n",
    "    # Drop the original column from kickstart\n",
    "    kickstart = kickstart.drop(col, axis=1)\n",
    "\n",
    "    # Concatenate the dummy variables with the main DataFrame\n",
    "    kickstart = pd.concat([kickstart, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "kickstart.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48494d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the dataset for numerical columns (int64 and float64)\n",
    "numerical_columns = kickstart.select_dtypes(include=[np.int64, np.float64])\n",
    "# Initializing the Isolation Forest model\n",
    "isolforest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "\n",
    "# Fitting the model on the numerical data\n",
    "pred = isolforest.fit_predict(numerical_columns)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred[pred[0] == -1].value_counts()\n",
    "indices = pred[pred[0] == -1].index\n",
    "\n",
    "print(len(indices)) #Number of outliers isolated\n",
    "\n",
    "#Removing outliers\n",
    "for i in indices:\n",
    "    if i in kickstart.index:\n",
    "        kickstart = kickstart.drop(i,axis = 0)\n",
    "\n",
    "#Reset index after removing        \n",
    "kickstart = kickstart.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cb8ce",
   "metadata": {},
   "source": [
    "# Checking Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348eb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe for the numerical columns\n",
    "numerical_cols = kickstart.select_dtypes(include=['float64','int64'])\n",
    "\n",
    "# Calculating skewness for each numerical column\n",
    "skewness = numerical_cols.apply(lambda x: skew(x.dropna()))  # dropna() to ignore NaN values\n",
    "\n",
    "#Printing Skewness\n",
    "skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de98bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There were 4 numerical columns that showed a bit skewness on both sides (Goal_usd, blurb_len_clean, create_to_deadline_days,\n",
    "#launched_at_yr)\n",
    "\n",
    "kickstart[\"log_goal\"] = np.log(kickstart[\"goal_usd\"])\n",
    "kickstart = kickstart.drop([\"goal_usd\"],axis=1)\n",
    "\n",
    "kickstart[\"log_blurb_len_clean\"] = np.log(kickstart[\"blurb_len_clean\"])\n",
    "kickstart = kickstart.drop([\"blurb_len_clean\"],axis=1)\n",
    "\n",
    "kickstart[\"log_create_to_deadline_days\"] = np.log(kickstart[\"create_to_deadline_days\"])\n",
    "kickstart = kickstart.drop([\"create_to_deadline_days\"],axis=1)\n",
    "\n",
    "kickstart[\"log_launched_at_yr\"] = np.log(kickstart[\"launched_at_yr\"])\n",
    "kickstart = kickstart.drop([\"launched_at_yr\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5baece",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e47117",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kickstart[\"state_successful\"]\n",
    "X = kickstart.drop([\"state_successful\"],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "feature_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "# Displaying feature importances\n",
    "print(feature_importances.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing columns that have less than 0.01 for all categories e.g. country\n",
    "kickstart = kickstart.drop([\"country_grouped_Other\",\"country_grouped_GB\",\"country_grouped_US\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictors and Target Variable\n",
    "X = kickstart.drop([\"state_successful\"],axis=1)\n",
    "y = kickstart[\"state_successful\"]\n",
    "\n",
    "# Define the columns to be converted\n",
    "columns_to_convert = [\"created_at_month\", \"created_at_day\", \"created_at_hr\",\n",
    "                      \"launched_at_month\", \"launched_at_day\",\"launched_at_hr\",\n",
    "                      \"deadline_month\", \"deadline_day\", \"deadline_hr\"\n",
    "                     ]\n",
    "\n",
    "# Use apply to convert each specified column to 'category' type\n",
    "X[columns_to_convert] = X[columns_to_convert].apply(lambda x: x.astype('category'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15d7ce",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54188f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test_train_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)\n",
    "\n",
    "#Random Forest Classifier\n",
    "clf = RandomForestClassifier(max_depth=20,min_samples_leaf=2,min_samples_split=3,n_estimators= 300, random_state=42,\n",
    "                             warm_start=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy using Random Forest Classifier:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b8ad0",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7b06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Initialize a RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator for further predictions\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fae513",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creating a Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(learning_rate=0.10,max_depth=3,n_estimators=205,random_state=42,min_samples_split=20\n",
    "                                   ,min_samples_leaf = 2)\n",
    "\n",
    "# Training the classifier\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "# Evaluating the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using Gradient Boosting Classifier is: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119f14d",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [100, 205, 200, 300, 250, 400],\n",
    "    'max_depth': [2, 3],\n",
    "    \"min_samples_leaf\" : [2,3,4,6],\n",
    "    \"min_samples_split\" : [2,4,6,8,10,15,20]\n",
    "}\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=gb_clf, param_grid=param_grid, n_jobs=-1, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \",grid_search.best_score_)\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "accuracy = best_clf.score(X_test, y_test)\n",
    "print(f\"Test set accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b836d",
   "metadata": {},
   "source": [
    "# Further Preprocessing for Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize days into beginning, middle, and end of the month\n",
    "def categorize_day_of_month(day):\n",
    "    if 1 <= day <= 10:\n",
    "        return 'Beginning'\n",
    "    elif 11 <= day <= 20:\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'End'\n",
    "\n",
    "# Apply this function to the day column\n",
    "kickstart['launched_at_day_group'] = kickstart['launched_at_day'].apply(categorize_day_of_month)\n",
    "kickstart['created_at_day_group'] = kickstart['created_at_day'].apply(categorize_day_of_month)\n",
    "kickstart['deadline_day_group'] = kickstart['deadline_day'].apply(categorize_day_of_month)\n",
    "\n",
    "kickstart = kickstart.drop(['launched_at_day','created_at_day','deadline_day'],axis=1)\n",
    "\n",
    "# Categorize months into quarters\n",
    "def categorize_month(month):\n",
    "\n",
    "    if 1 <= month <= 3:\n",
    "        return 'Q1'\n",
    "\n",
    "    elif 4 <= month <= 6:\n",
    "        return 'Q2'\n",
    "\n",
    "    elif 7 <= month <= 9:\n",
    "        return 'Q3'\n",
    "\n",
    "    else:\n",
    "        return 'Q4'\n",
    "\n",
    "    \n",
    "# Apply this function to the months column\n",
    "kickstart['launched_at_month_quarter'] = kickstart['launched_at_month'].apply(categorize_month)\n",
    "kickstart['created_at_month_quarter'] = kickstart['created_at_month'].apply(categorize_month)\n",
    "kickstart['deadline_month_quarter'] = kickstart['deadline_month'].apply(categorize_month)\n",
    "\n",
    "kickstart = kickstart.drop(['launched_at_month','created_at_month','deadline_month'],axis=1)\n",
    "\n",
    "def categorize_hour(hour):\n",
    "\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Early Morning'\n",
    "\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'Afternoon/Evening'\n",
    "\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "\n",
    "# Apply this function to the hours column\n",
    "kickstart['launched_at_hour_group'] = kickstart['launched_at_hr'].apply(categorize_hour)\n",
    "kickstart['created_at_hour_group'] = kickstart['created_at_hr'].apply(categorize_hour)\n",
    "kickstart['deadline_hour_group'] = kickstart['deadline_hr'].apply(categorize_hour)\n",
    "\n",
    "kickstart = kickstart.drop(['launched_at_hr','created_at_hr','deadline_hr'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad630b",
   "metadata": {},
   "source": [
    "# Dummifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104db79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kickstart_df = kickstart.copy()\n",
    "\n",
    "for col in kickstart_df.select_dtypes(include=['object']).columns:\n",
    "\n",
    "    # Get unique values of the column\n",
    "    unique_values = kickstart_df[col].unique()\n",
    "\n",
    "    # Initialize an empty DataFrame for the dummies of this column\n",
    "    column_dummies = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for value in unique_values:\n",
    "\n",
    "        # Determine the name of the new dummy column\n",
    "        dummy_col_name = f\"{col}_{value}\"\n",
    "\n",
    "        # Create a dummy column for the value\n",
    "        kickstart_df[dummy_col_name] = (kickstart_df[col] == value).astype(int)\n",
    "\n",
    "\n",
    "        # If the value ends with 'others' or 'weekend', drop the first dummy\n",
    "        if value.endswith('others') or value.endswith('weekend'):\n",
    "            kickstart_df.drop(dummy_col_name, axis=1, inplace=True)\n",
    "\n",
    "    # Drop the original column\n",
    "    kickstart_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "kickstart_df = kickstart_df.drop([\"category_Other\",\"deadline_day_group_End\",\"created_at_day_group_End\"\n",
    "                                 ,\"launched_at_day_group_End\",\"launched_at_month_quarter_Q4\",\"created_at_month_quarter_Q4\",\"deadline_month_quarter_Q4\"\n",
    "                                 ,\"launched_at_hour_group_Early Morning\",\"created_at_hour_group_Early Morning\",\"deadline_hour_group_Early Morning\"\n",
    "                                 ,],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb62bd",
   "metadata": {},
   "source": [
    "# Checking Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a81de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = kickstart_df.corr()\n",
    "\n",
    "# Filter out pairs with correlation greater than 0.8\n",
    "\n",
    "high_correlation_pairs = [(i, j) for i in correlation_matrix.columns for j in correlation_matrix.columns \n",
    "                          if (i != j) and (abs(correlation_matrix[i][j]) >= 0.8)]\n",
    "\n",
    "\n",
    "print(\"Pairs with high correlation (> 0.8):\")\n",
    "print(high_correlation_pairs)\n",
    "\n",
    "#Pairs with high correlation (> 0.8):\n",
    "[('launched_at_hour_group_Morning', 'deadline_hour_group_Morning'), ('deadline_hour_group_Morning', 'launched_at_hour_group_Morning')]\n",
    "\n",
    "kickstart_df = kickstart_df.drop([\"launched_at_hour_group_Morning\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aac107",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe6cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing predictors\n",
    "X = kickstart_df.drop([\"state_successful\"],axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_std = scaler.fit_transform(X)\n",
    "X_std = pd.DataFrame(X_std,columns = X.columns)\n",
    "\n",
    "#Train_test_split\n",
    "X_std_train, X_std_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the MLPClassifier\n",
    "mlp = MLPClassifier(alpha=0.0001,hidden_layer_sizes=(32, 16), max_iter=1000, activation='logistic', solver='adam', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_std_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = mlp.predict(X_std_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Accuracy using ANN is: {accuracy_score(y_test, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59abe47",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(64, 32), (32, 16), (128, 64)],  # Structure of hidden layers\n",
    "    'solver': ['adam', 'sgd'],  # Solver for weight optimization\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # L2 penalty (regularization term) parameter\n",
    "}\n",
    "mlp = MLPClassifier(max_iter = 10000, random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit the grid search to the scaled training data\n",
    "grid_search.fit(X_std_train, y_train)\n",
    "\n",
    "# Get the best parameters and corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict using the best model\n",
    "y_test_pred = best_model.predict(X_std_test)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_mlp = grid_search.best_estimator_\n",
    "test_score = best_mlp.score(X_std_test, y_test)\n",
    "print(f\"Test Accuracy using ANN : {test_score * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b0b9a",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42,C=100, penalty='l1',solver='liblinear')\n",
    "\n",
    "# Training the model\n",
    "log_reg.fit(X_std_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = log_reg.predict(X_std_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy using Logistic regression is: {accuracy}\")\n",
    "print(\"Classification Report for Logistic Regression is :\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c2bf7",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d408186",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'penalty': ['l1', 'l2'],       # Type of regularization\n",
    "    'solver': ['liblinear', 'saga'] # Algorithm to use for optimization\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_std_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \",grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_logit = grid_search.best_estimator_\n",
    "test_score = best_logit.score(X_std_test, y_test)\n",
    "print(f\"Test Accuracy : {test_score * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c08669",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79400465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=30)  # You can change n_neighbors as needed\n",
    "\n",
    "# Training the model\n",
    "knn.fit(X_std_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = knn.predict(X_std_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy using KNN is : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4322112",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da05c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kmodes\n",
    "import matplotlib.pyplot as plt\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kmodes import kprototypes\n",
    "from sklearn.metrics import silhouette_score\n",
    "from kmodes.kprototypes import KPrototypes, matching_dissim, euclidean_dissim\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from scipy.stats import f\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib import pyplot\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba99f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "kickstart = pd.read_excel(\"E:/Users/pc/Downloads/Mining proj/Induvidual proj/Kickstarter.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text (basic steps)\n",
    "kickstart['name'] = kickstart['name'].fillna('').str.lower()\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(kickstart['name'])\n",
    "\n",
    "# Function to impute missing values\n",
    "def impute_category(row):\n",
    "    if pd.isna(row['category']):\n",
    "        # Compute similarity\n",
    "        similarities = cosine_similarity(tfidf_matrix, tfidf_matrix[row.name])\n",
    "        \n",
    "        # Get indices of most similar rows\n",
    "        similar_indices = np.argsort(similarities[:,0])[-3:-1]  # Adjust as needed\n",
    "\n",
    "        # Impute using the mode of the most similar rows\n",
    "        similar_categories = kickstart.iloc[similar_indices]['category']\n",
    "        if not similar_categories.empty:\n",
    "            mode_result = similar_categories.mode()\n",
    "            if not mode_result.empty:\n",
    "                return mode_result.iloc[0]\n",
    "            else:\n",
    "                return np.nan\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return row['category']\n",
    "\n",
    "# Apply imputation\n",
    "kickstart['category'] = kickstart.apply(impute_category, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ee4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting goal to goal_usd by multiplying with static_usd_rate\n",
    "kickstart[\"goal_usd\"] = kickstart[\"goal\"] * kickstart[\"static_usd_rate\"]\n",
    "\n",
    "#Drop invalid predictors\n",
    "kickstart = kickstart.drop([\n",
    "    'id', 'name', 'goal', 'pledged', 'deadline', 'created_at', 'launched_at',\n",
    "    'usd_pledged', 'static_usd_rate', 'name_len', 'blurb_len',\n",
    "    'state_changed_at', 'state_changed_at_month', 'state_changed_at_day',\n",
    "    'state_changed_at_yr', 'state_changed_at_hr'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc83a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping only rows with state = successful or failed\n",
    "kickstart = kickstart[kickstart['state'].isin(['successful', 'failed'])]\n",
    "kickstart = kickstart.dropna()\n",
    "kickstart = kickstart.reset_index(drop=True)\n",
    "\n",
    "#Feature engineering to create \"create_to_deadline_days\"\n",
    "kickstart[\"create_to_deadline_days\"] = kickstart[\"create_to_launch_days\"] + kickstart[\"launch_to_deadline_days\"]\n",
    "\n",
    "#Dropiing invalid predictors\n",
    "kickstart = kickstart.drop(['launch_to_state_change_days'],axis=1)\n",
    "kickstart = kickstart.drop([\"state_changed_at_weekday\"],axis=1)\n",
    "kickstart = kickstart.drop([\"currency\",\"disable_communication\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the dataset for numerical columns (int64 and float64)\n",
    "numerical_columns = kickstart.select_dtypes(include=[np.int64, np.float64])\n",
    "# Initializing the Isolation Forest model\n",
    "isolforest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "\n",
    "# Fitting the model on the numerical data\n",
    "pred = isolforest.fit_predict(numerical_columns)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred[pred[0] == -1].value_counts()\n",
    "indices = pred[pred[0] == -1].index\n",
    "\n",
    "print(len(indices)) #Number of outliers isolated\n",
    "\n",
    "#Removing outliers\n",
    "for i in indices:\n",
    "    if i in kickstart.index:\n",
    "        kickstart = kickstart.drop(i,axis = 0)\n",
    "\n",
    "#Reset index after removing        \n",
    "kickstart = kickstart.reset_index(drop=True)\n",
    "kickstart.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataframe into categorical and numerical dataframe\n",
    "categorical = kickstart.drop([\"name_len_clean\",\"blurb_len_clean\",\"goal_usd\",\"create_to_deadline_days\",\"create_to_launch_days\",\"launch_to_deadline_days\",\"backers_count\"],axis=1)\n",
    "numerical = kickstart.drop(categorical,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc045318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler on numerical features\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "numerical_std = scaler.fit_transform(numerical)\n",
    "numerical_std = pd.DataFrame(numerical_std, columns =numerical.columns)\n",
    "\n",
    "# Merge data back\n",
    "X_std = pd.merge(numerical_std, categorical, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02519fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vieweing merged dataframe\n",
    "X_std.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed24f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kmodes\n",
    "cat_num_cols = [\"created_at_month\", \"created_at_day\", \"created_at_hr\", \"created_at_yr\", \"created_at_weekday\", \n",
    "                      \"launched_at_month\", \"launched_at_day\", \"launched_at_yr\",\"launched_at_weekday\",\n",
    "                      \"launched_at_hr\", \"deadline_month\", \"deadline_day\", \"deadline_hr\",\"deadline_weekday\",\n",
    "                      \"deadline_yr\",\"state\",\"spotlight\",\"staff_pick\",\"category\",\"country\"]\n",
    "\n",
    "# Convert these columns to 'category' dtype\n",
    "for col in cat_num_cols:\n",
    "    X_std[col] = X_std[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471871ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running KProtoypes\n",
    "kmixed = KPrototypes(n_clusters=4,random_state=50)\n",
    "cluster = kmixed.fit_predict(X_std, categorical=[7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26])\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print (pd.DataFrame(kmixed.cluster_centroids_, columns=X_std.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing cost which is a combination of cluster cohesion and seperation\n",
    "costs = []\n",
    "K = range(2,7)\n",
    "#Run loop\n",
    "for num_clusters in K:\n",
    "    kproto = KPrototypes(n_clusters=num_clusters, init='Cao', n_init=5,random_state=50)\n",
    "    clusters = kproto.fit_predict(X_std, categorical=[7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26])\n",
    "    costs.append(kproto.cost_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff33eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting cost v/s number of clusters\n",
    "plt.plot(K, costs, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Elbow Method For Optimal k in K-Prototypes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dff08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply .describe() to each cluster\n",
    "cluster_descriptions = []\n",
    "for i in range(kmixed.n_clusters):\n",
    "    cluster_data = X_std[clusters == i]\n",
    "    cluster_descriptions.append(cluster_data.describe(include='all'))  # include='all' to get statistics for categorical columns as well\n",
    "\n",
    "# Display the descriptive statistics for each cluster\n",
    "print(\"\\nCluster Descriptions:\")\n",
    "for i, desc in enumerate(cluster_descriptions):\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971166e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the mixed distance\n",
    "def mixed_distance(a, b, categorical=None, alpha=0.01):\n",
    "    if categorical is None:\n",
    "        num_score = kprototypes.euclidean_dissim(a, b)\n",
    "        return num_score\n",
    "    else:\n",
    "        cat_index = categorical\n",
    "        a_cat = [a[index] for index in cat_index]\n",
    "        b_cat = [b[index] for index in cat_index]\n",
    "        a_num = [a[index] for index in range(len(a)) if index not in cat_index]\n",
    "        b_num = [b[index] for index in range(len(b)) if index not in cat_index]\n",
    "        \n",
    "        a_cat = np.array(a_cat).reshape(1, -1)\n",
    "        b_cat = np.array(b_cat).reshape(1, -1)\n",
    "        a_num = np.array(a_num).reshape(1, -1)\n",
    "        b_num = np.array(b_num).reshape(1, -1)\n",
    "        \n",
    "        cat_score = kprototypes.matching_dissim(a_cat, b_cat)\n",
    "        num_score = kprototypes.euclidean_dissim(a_num, b_num)\n",
    "        return cat_score + num_score * alpha\n",
    "\n",
    "# Function to compute the distance matrix\n",
    "def dm_prototypes(dataset, categorical=None, alpha=0.1):\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        dataset = dataset.values\n",
    "    len_dataset = len(dataset)\n",
    "    distance_matrix = np.zeros((len_dataset, len_dataset))\n",
    "    for i in range(len_dataset):\n",
    "        for j in range(len_dataset):\n",
    "            distance_matrix[i][j] = mixed_distance(dataset[i], dataset[j], categorical=categorical, alpha=alpha)\n",
    "    return distance_matrix\n",
    "\n",
    "# And 'categorical_columns' with indices of your categorical columns\n",
    "categorical_columns = [7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]\n",
    "\n",
    "# Calculate the custom distance matrix for your dataset\n",
    "distance_matrix = dm_prototypes(X_std, categorical=categorical_columns, alpha=0.1)\n",
    "\n",
    "# Range of potential clusters\n",
    "cluster_range = range(2, 7)  # for example, from 2 to 6 clusters\n",
    "\n",
    "# Silhouette scores list\n",
    "silhouette_scores = []\n",
    "\n",
    "# Calculating silhouette scores for different number of clusters\n",
    "for n_clusters in cluster_range:\n",
    "    kmixed = KPrototypes(n_clusters=n_clusters, random_state=50)\n",
    "    cluster_labels = kmixed.fit_predict(X_std, categorical=categorical_columns)\n",
    "    silhouette_avg = silhouette_score(distance_matrix, cluster_labels, metric='precomputed')\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plotting the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559613aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = calinski_harabasz_score(numerical, cluster_labels) #calculates f-score\n",
    "\n",
    "# Calculate p-value\n",
    "df1 = 3 # df1 = k-1\n",
    "df2 = 1261 # df2 = n-k\n",
    "pvalue = 1-f.cdf(score, df1, df2)\n",
    "#getting very small p-value\n",
    "\n",
    "# Finding optimal K\n",
    "for i in range (2,7):    \n",
    "    df1=i-1\n",
    "    df2=22-i\n",
    "    kmeans = KMeans(n_clusters=i,n_init=\"auto\")\n",
    "    model = kmeans.fit(numerical)\n",
    "    labels = model.labels_\n",
    "    score = calinski_harabasz_score(numerical, cluster_labels)\n",
    "    print(i,'F-score:',score)\n",
    "    print(i,'p-value:',1-f.cdf(score, df1, df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25252fb1",
   "metadata": {},
   "source": [
    "# Preprocessing for Other Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd22a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify major countries with a significant number of projects\n",
    "major_countries = ['US', 'GB', 'CA']  # Based on the initial data overview\n",
    "\n",
    "# Step 2: Create a function to categorize countries\n",
    "def categorize_country(country):\n",
    "\n",
    "    if country in major_countries:\n",
    "\n",
    "        return country\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 'Other'\n",
    "    \n",
    "\n",
    "# Step 3: Apply the categorization function to the 'country' column\n",
    "\n",
    "kickstart['country_grouped'] = kickstart['country'].apply(categorize_country)\n",
    "\n",
    "# Display the first few rows of the dataset to verify the changes\n",
    "\n",
    "kickstart = kickstart.drop([\"country\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfcf3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using top 8 categories we are covering 80% of the distribution, so the rest will be classified as others.\n",
    "\n",
    "# Extracting the top 8 categories from the 'category' column\n",
    "top_8_categories = kickstart['category'].value_counts().nlargest(8).index.tolist()\n",
    "\n",
    "# Categorization function to categorize based on the extracted top 8 categories\n",
    "def categorize_based_on_top_8(category):\n",
    "    if category in top_8_categories:\n",
    "        return category\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the categorization function to the 'category' column\n",
    "kickstart['category_grouped_top_8'] = kickstart['category'].apply(categorize_based_on_top_8)\n",
    "\n",
    "# Display the first few rows of the dataset to verify the changes\n",
    "kickstart[['category', 'category_grouped_top_8']].head()\n",
    "\n",
    "#Removing and assigning grouped values to the original category column\n",
    "kickstart = kickstart.drop([\"category\"],axis=1)\n",
    "kickstart[\"category\"] = kickstart[\"category_grouped_top_8\"]\n",
    "kickstart = kickstart.drop([\"category_grouped_top_8\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65042fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize days into beginning, middle, and end of the month\n",
    "\n",
    "def categorize_day_of_month(day):\n",
    "\n",
    "    if 1 <= day <= 10:\n",
    "\n",
    "        return 'Beginning'\n",
    "\n",
    "    elif 11 <= day <= 20:\n",
    "\n",
    "        return 'Middle'\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 'End'\n",
    "\n",
    "# Apply this function to the day column\n",
    "\n",
    "kickstart['launched_at_day_group'] = kickstart['launched_at_day'].apply(categorize_day_of_month)\n",
    "\n",
    "kickstart['created_at_day_group'] = kickstart['created_at_day'].apply(categorize_day_of_month)\n",
    "\n",
    "kickstart['deadline_day_group'] = kickstart['deadline_day'].apply(categorize_day_of_month)\n",
    "\n",
    "\n",
    "\n",
    "kickstart = kickstart.drop(['launched_at_day','created_at_day','deadline_day'],axis=1)\n",
    "\n",
    "# Categorize months into quarters\n",
    "\n",
    "def categorize_month(month):\n",
    "\n",
    "    if 1 <= month <= 3:\n",
    "\n",
    "        return 'Q1'\n",
    "\n",
    "    elif 4 <= month <= 6:\n",
    "\n",
    "        return 'Q2'\n",
    "\n",
    "    elif 7 <= month <= 9:\n",
    "\n",
    "        return 'Q3'\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 'Q4'\n",
    "\n",
    "    \n",
    "kickstart['launched_at_month_quarter'] = kickstart['launched_at_month'].apply(categorize_month)\n",
    "\n",
    "kickstart['created_at_month_quarter'] = kickstart['created_at_month'].apply(categorize_month)\n",
    "\n",
    "kickstart['deadline_month_quarter'] = kickstart['deadline_month'].apply(categorize_month)\n",
    "\n",
    "kickstart = kickstart.drop(['launched_at_month','created_at_month','deadline_month'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def categorize_hour(hour):\n",
    "\n",
    "    if 0 <= hour < 6:\n",
    "\n",
    "        return 'Early Morning'\n",
    "\n",
    "    elif 6 <= hour < 12:\n",
    "\n",
    "        return 'Morning'\n",
    "\n",
    "    elif 12 <= hour < 18:\n",
    "\n",
    "        return 'Afternoon/Evening'\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 'Night'\n",
    "\n",
    "\n",
    "# Apply this function to the 'launched_at_hr' column\n",
    "\n",
    "kickstart['launched_at_hour_group'] = kickstart['launched_at_hr'].apply(categorize_hour)\n",
    "\n",
    "kickstart['created_at_hour_group'] = kickstart['created_at_hr'].apply(categorize_hour)\n",
    "\n",
    "kickstart['deadline_hour_group'] = kickstart['deadline_hr'].apply(categorize_hour)\n",
    "\n",
    "kickstart = kickstart.drop(['launched_at_hr','created_at_hr','deadline_hr'],axis=1)\n",
    "\n",
    "\n",
    "#Create a copy of kickstart dataframe\n",
    "kickstart_df = kickstart.copy()\n",
    "\n",
    "\n",
    "for col in kickstart_df.select_dtypes(include=['object']).columns:\n",
    "\n",
    "    # Get unique values of the column\n",
    "\n",
    "    unique_values = kickstart_df[col].unique()\n",
    "\n",
    "    # Initialize an empty DataFrame for the dummies of this column\n",
    "\n",
    "    column_dummies = pd.DataFrame()\n",
    "\n",
    "    for value in unique_values:\n",
    "\n",
    "        # Determine the name of the new dummy column\n",
    "\n",
    "        dummy_col_name = f\"{col}_{value}\"\n",
    "\n",
    "        # Create a dummy column for the value\n",
    "        \n",
    "        kickstart_df[dummy_col_name] = (kickstart_df[col] == value).astype(int)\n",
    "\n",
    "        # If the value ends with 'others' or 'weekend', drop the first dummy\n",
    "        \n",
    "        if value.endswith('others') or value.endswith('weekend'):\n",
    "\n",
    "            kickstart_df.drop(dummy_col_name, axis=1, inplace=True)\n",
    "\n",
    "    # Drop the original column\n",
    "\n",
    "    kickstart_df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = kickstart_df\n",
    "\n",
    "X_std = scaler.fit_transform(X)\n",
    "X_std = pd.DataFrame(X_std,columns = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f8732d",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd573f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = pd.DataFrame(X_std,columns=X.columns)\n",
    "kmeans = KMeans(n_clusters=4, random_state=50)  # 4 clusters\n",
    "clusters = kmeans.fit_predict(X_std)\n",
    "\n",
    "# Add the cluster labels to your original dataframe for further analysis\n",
    "X_std['cluster'] = clusters\n",
    "\n",
    "# Analyze the results\n",
    "with pd.option_context(\"display.max_rows\",None,\"display.max_columns\",None):\n",
    "    print(X_std.groupby('cluster').mean())  # Examining the mean values of features for each cluster\n",
    "\n",
    "# Basic visualization (only works if we have 2 or 3 features)\n",
    "if X_std.shape[1] == 2:\n",
    "    plt.scatter(X_std[:, 0], X_std[:, 1], c=clusters, cmap='viridis')\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n",
    "    plt.title(\"KMeans Clustering\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette score for K-means for 4 clusters\n",
    "score = silhouette_score(X_std, clusters)\n",
    "\n",
    "print(\"Silhouette Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1abd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F-score\n",
    "score = calinski_harabasz_score(X_std, clusters) #calculates f-score\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d1f08b",
   "metadata": {},
   "source": [
    "# Hiearchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agglomerative/Hierarchical clustering with complete linkage\n",
    "cluster = AgglomerativeClustering(n_clusters=4,linkage = \"complete\", metric = \"euclidean\")\n",
    "cluster.fit_predict(X_std)\n",
    "ams = cluster.labels_\n",
    "\n",
    "# Using scipy's linkage for dendrogram\n",
    "linked = linkage(X_std, 'complete')\n",
    "\n",
    "# Plotting the dendrogram\n",
    "plt.figure(figsize=(16, 7))\n",
    "dendrogram(linked, orientation='top', labels=ams, distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Complete Linkage)')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13546a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Silhouette Score for hiearchical clustering\n",
    "silhouette_avg = silhouette_score(X_std, ams)\n",
    "print(\"Silhouette Score: \", silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F-score\n",
    "score = calinski_harabasz_score(X_std, ams) #calculates f-score\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "X_pca = pd.DataFrame(X_pca, columns =['PC1', 'PC2']) \n",
    "\n",
    "#Visualizing components\n",
    "pyplot.scatter(X_pca['PC1'], X_pca['PC2'])\n",
    "pyplot.xlabel(\"PC 1\")\n",
    "pyplot.ylabel(\"PC 2\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf436103",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=2.5, min_samples=30)  # These parameters can be tuned\n",
    "clusters = dbscan.fit_predict(X_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_silhouette = silhouette_score(X_std, clusters) if len(set(clusters)) > 1 else 0\n",
    "print(dbscan_silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b79b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F-score\n",
    "score = calinski_harabasz_score(X_std, clusters) #calculates f-score\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248be19",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import floatx\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_std = pd.DataFrame(X_std)\n",
    "X_std = np.asarray(X_std).astype(np.float32)  # Replace with your dataset\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_std_train, X_std_test = train_test_split(X_std, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the size of the encoded representations\n",
    "encoding_dim = 32  # Adjust as needed\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(X_std_train.shape[1],))\n",
    "\n",
    "# Define the encoding layer\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Define the decoding layer\n",
    "decoded = Dense(X_std_train.shape[1], activation='relu')(encoded)\n",
    "\n",
    "# Build the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_std_train, X_std_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_std_test, X_std_test))\n",
    "\n",
    "# Build the encoder model for dimensionality reduction\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# Encode the test data\n",
    "encoded_data = encoder.predict(X_std_test)\n",
    "\n",
    "# Apply K-Means clustering on the encoded data\n",
    "kmeans = KMeans(n_clusters=4, random_state=50)\n",
    "clusters = kmeans.fit_predict(encoded_data)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "score = silhouette_score(encoded_data, clusters)\n",
    "print('Silhouette Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing clustering using auto encoders\n",
    "\n",
    "# Further reduce the dimensionality to 2D for visualization using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(encoded_data)\n",
    "\n",
    "# Plot the resulting clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=clusters, cmap='viridis', marker='o')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Cluster Visualization')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
